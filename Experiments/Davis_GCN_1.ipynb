{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "\n",
    "# !pip install torch_sparse\n",
    "# !pip install torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os, json \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import global_mean_pool as gep\n",
    "from torch_geometric import data as DATA\n",
    "from torch_geometric.data import InMemoryDataset, Batch\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded feature information \n",
    "# Reference: https://www.bioinfor.com/amino-acid/ and https://www.sigmaaldrich.com/life-science/metabolomics/learning-center/amino-acid-reference-chart.html\n",
    "\n",
    "residue_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X']\n",
    "\n",
    "aliphatic_residues_table = ['A', 'I', 'L', 'M', 'V']\n",
    "aromatic_residues_table = ['F', 'W', 'Y']\n",
    "polar_neutral_residues_table = ['C', 'N', 'Q', 'S', 'T']\n",
    "acidic_charged_residues_table = ['D', 'E']\n",
    "basic_charged_residues_table = ['H', 'K', 'R']\n",
    "\n",
    "weight_table = {'A': 71.08, 'C': 103.15, 'D': 115.09, 'E': 129.12, 'F': 147.18, 'G': 57.05, 'H': 137.14,\n",
    "                    'I': 113.16, 'K': 128.18, 'L': 113.16, 'M': 131.20, 'N': 114.11, 'P': 97.12, 'Q': 128.13,\n",
    "                    'R': 156.19, 'S': 87.08, 'T': 101.11, 'V': 99.13, 'W': 186.22, 'Y': 163.18}\n",
    "\n",
    "hydrophobic_ph2_table = {'A': 47, 'C': 52, 'D': -18, 'E': 8, 'F': 92, 'G': 0, 'H': -42, 'I': 100,\n",
    "                             'K': -37, 'L': 100, 'M': 74, 'N': -41, 'P': -46, 'Q': -18, 'R': -26, 'S': -7,\n",
    "                             'T': 13, 'V': 79, 'W': 84, 'Y': 49}\n",
    "\n",
    "hydrophobic_ph7_table = {'A': 41, 'C': 49, 'D': -55, 'E': -31, 'F': 100, 'G': 0, 'H': 8, 'I': 99,\n",
    "                             'K': -23, 'L': 97, 'M': 74, 'N': -28, 'P': -46, 'Q': -10, 'R': -14, 'S': -5,\n",
    "                             'T': 13, 'V': 76, 'W': 97, 'Y': 63}\n",
    "\n",
    "pl_table = {'A': 6.00, 'C': 5.07, 'D': 2.77, 'E': 3.22, 'F': 5.48, 'G': 5.97, 'H': 7.59,\n",
    "                'I': 6.02, 'K': 9.74, 'L': 5.98, 'M': 5.74, 'N': 5.41, 'P': 6.30, 'Q': 5.65,\n",
    "                'R': 10.76, 'S': 5.68, 'T': 5.60, 'V': 5.96, 'W': 5.89, 'Y': 5.96}\n",
    "\n",
    "pka_table = {'A': 2.34, 'C': 1.96, 'D': 1.88, 'E': 2.19, 'F': 1.83, 'G': 2.34, 'H': 1.82, 'I': 2.36,\n",
    "                 'K': 2.18, 'L': 2.36, 'M': 2.28, 'N': 2.02, 'P': 1.99, 'Q': 2.17, 'R': 2.17, 'S': 2.21,\n",
    "                 'T': 2.09, 'V': 2.32, 'W': 2.83, 'Y': 2.32}\n",
    "\n",
    "pkb_table = {'A': 9.69, 'C': 10.28, 'D': 9.60, 'E': 9.67, 'F': 9.13, 'G': 9.60, 'H': 9.17,\n",
    "                 'I': 9.60, 'K': 8.95, 'L': 9.60, 'M': 9.21, 'N': 8.80, 'P': 10.60, 'Q': 9.13,\n",
    "                 'R': 9.04, 'S': 9.15, 'T': 9.10, 'V': 9.62, 'W': 9.39, 'Y': 9.62}\n",
    "\n",
    "pkx_table = {'A': 0.00, 'C': 8.18, 'D': 3.65, 'E': 4.25, 'F': 0.00, 'G': 0, 'H': 6.00,\n",
    "                 'I': 0.00, 'K': 10.53, 'L': 0.00, 'M': 0.00, 'N': 0.00, 'P': 0.00, 'Q': 0.00,\n",
    "                 'R': 12.48, 'S': 0.00, 'T': 0.00, 'V': 0.00, 'W': 0.00, 'Y': 0.00}\n",
    "\n",
    "atom_table = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', \n",
    "              'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se','Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', \n",
    "              'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'X']\n",
    "\n",
    "count_table = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Normalize\n",
    "\n",
    "def normalize_dict(dic):\n",
    "    max_ = dic[max(dic, key=dic.get)]\n",
    "    min_ = dic[min(dic, key=dic.get)]\n",
    "    interval = float(max_) - float(min_)\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        dic[key] = (dic[key] - min_) / interval\n",
    "        \n",
    "    dic['X'] = (max_ + min_) / 2.0 # For unknown \n",
    "    return dic\n",
    "\n",
    "weight_table = normalize_dict(weight_table)\n",
    "pka_table = normalize_dict(pka_table)\n",
    "pkb_table = normalize_dict(pkb_table)\n",
    "pkx_table = normalize_dict(pkx_table)\n",
    "pl_table = normalize_dict(pl_table)\n",
    "hydrophobic_ph2_table = normalize_dict(hydrophobic_ph2_table)\n",
    "hydrophobic_ph7_table = normalize_dict(hydrophobic_ph7_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epochs:  1000\n"
     ]
    }
   ],
   "source": [
    "# Parameter settings \n",
    "\n",
    "dataset = 'davis' \n",
    "\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "NUM_PROT_FEATURES = 33\n",
    "NUM_MOL_FEATURES = 78\n",
    "NUM_RES_PROPERTIES = 12\n",
    "\n",
    "DROPOUT = 0.2\n",
    "\n",
    "print('Learning rate: ', LR)\n",
    "print('Epochs: ', NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory creation for models/results\n",
    "\n",
    "models_dir = 'models'\n",
    "results_dir = 'results'\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture definition \n",
    "\n",
    "# 3-layer, GCN-based model\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro=NUM_PROT_FEATURES, num_features_mol=NUM_MOL_FEATURES, output_dim=128, dropout=DROPOUT):\n",
    "        super(GCNNet, self).__init__()\n",
    "        print('GCNNet Loaded')\n",
    "        \n",
    "        # Output layer\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # Mol convolutional layers\n",
    "        self.mol_conv1 = GCNConv(num_features_mol, num_features_mol)\n",
    "        # self.mol_conv2 = GCNConv(num_features_mol, num_features_mol*2)\n",
    "        # self.mol_conv3 = GCNConv(num_features_mol*2, num_features_mol*4)\n",
    "        \n",
    "        # Mol fully connected layers\n",
    "        self.mol_fc1 = torch.nn.Linear(num_features_mol, 1024)\n",
    "        self.mol_fc2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        # Protein convolutional layers\n",
    "        self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        #self.pro_conv2 = GCNConv(num_features_pro, num_features_pro*2)\n",
    "        # self.pro_conv3 = GCNConv(num_features_pro*2, num_features_pro*4)\n",
    "        \n",
    "        # Protein fully connected layers\n",
    "        self.pro_fc1 = torch.nn.Linear(num_features_pro, 1024)\n",
    "        self.pro_fc2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        # Other\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Final layers\n",
    "        self.fc1 = nn.Linear(2*output_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, self.n_output)\n",
    "\n",
    "    def forward(self, data_mol, data_pro):\n",
    "        # Molecule info\n",
    "        mol_data, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
    "        \n",
    "        # Protein info\n",
    "        target_data, target_edge_index, target_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
    "        \n",
    "        # Protein\n",
    "        \n",
    "        x_p = self.pro_conv1(target_data, target_edge_index)\n",
    "        x_p = self.relu(x_p)\n",
    "\n",
    "        x_p = self.pro_conv2(x_p, target_edge_index)\n",
    "        x_p = self.relu(x_p)\n",
    "\n",
    "        x_p = self.pro_conv3(x_p, target_edge_index)\n",
    "        x_p = self.relu(x_p)\n",
    "\n",
    "        x_p = gep(x_p, target_batch) # Pooling\n",
    "\n",
    "        x_p = self.relu(self.pro_fc1(x_p))\n",
    "        x_p = self.dropout(x_p)\n",
    "        \n",
    "        x_p = self.pro_fc2(x_p)\n",
    "        x_p = self.dropout(x_p)\n",
    "        \n",
    "        # Molecule\n",
    "        \n",
    "        x_m = self.mol_conv1(mol_data, mol_edge_index)\n",
    "        x_m = self.relu(x_m)\n",
    "        \n",
    "        x_m = self.mol_conv2(x_m, mol_edge_index)\n",
    "        x_m = self.relu(x_m)\n",
    "        \n",
    "        x_m = self.mol_conv3(x_m, mol_edge_index)\n",
    "        x_m = self.relu(x_m)\n",
    "        \n",
    "        x_m = gep(x_m, mol_batch) # Pooling\n",
    "\n",
    "        x_m = self.relu(self.mol_fc1(x_m))\n",
    "        x_m = self.dropout(x_m)\n",
    "        \n",
    "        x_m = self.mol_fc2(x_m)\n",
    "        x_m = self.dropout(x_m)\n",
    "        \n",
    "        # Concatenation\n",
    "\n",
    "        x_c = torch.cat((x_m, x_p), 1)\n",
    "        \n",
    "        x_c = self.fc1(x_c)\n",
    "        x_c = self.relu(x_c)\n",
    "        x_c = self.dropout(x_c)\n",
    "        \n",
    "        x_c = self.fc2(x_c)\n",
    "        x_c = self.relu(x_c)\n",
    "        x_c = self.dropout(x_c)\n",
    "        \n",
    "        out = self.out(x_c)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training and featurization \n",
    "\n",
    "# Create one hot encoding with k choices\n",
    "def one_k_encoding(x, choices):\n",
    "    return list(map(lambda s: x == s, choices))\n",
    "\n",
    "# If not in choices, set as last choice (for X residue)\n",
    "def one_not_k_encoding(x, choices):\n",
    "    if x not in choices:\n",
    "        x = choices[-1]\n",
    "    return list(map(lambda s: x == s, choices))\n",
    "\n",
    "def sequence_feature(protein_seq):\n",
    "    protein_property = np.zeros((len(protein_seq), NUM_RES_PROPERTIES))\n",
    "    protein_one_hot = np.zeros((len(protein_seq), len(residue_table)))\n",
    "    \n",
    "    for i in range(len(protein_seq)):\n",
    "        protein_property[i,] = residue_features(protein_seq[i]) # Get the chemical properties\n",
    "        protein_one_hot[i,] = one_k_encoding(protein_seq[i], residue_table) # Get the residue identity \n",
    "        \n",
    "    return np.concatenate((protein_property, protein_one_hot), axis=1)\n",
    "\n",
    "def protein_to_feature(protein_key, protein_seq):\n",
    "    feature = sequence_feature(protein_seq)\n",
    "    return feature\n",
    "\n",
    "def protein_to_graph(protein_key, protein_seq, contact_dir):\n",
    "    protein_edges = []\n",
    "    protein_len = len(protein_seq)\n",
    "    \n",
    "    contact_file = os.path.join(contact_dir, protein_key + '.npy')\n",
    "    contact_map = np.load(contact_file)\n",
    "    contact_map += np.matrix(np.eye(contact_map.shape[0]))\n",
    "    \n",
    "    row, col = np.where(contact_map >= 0.5) # Apply the contact map 0.5 threshold\n",
    "    for i, j in zip(row, col):\n",
    "        protein_edges.append([i, j]) # Get all the edges\n",
    "    protein_feature = protein_to_feature(protein_key, protein_seq)\n",
    "    protein_edges = np.array(protein_edges)\n",
    "    \n",
    "    return protein_len, protein_feature, protein_edges\n",
    "\n",
    "def residue_features(residue):\n",
    "    numeric_properties = [weight_table[residue],\n",
    "                          hydrophobic_ph2_table[residue], \n",
    "                          hydrophobic_ph7_table[residue],\n",
    "                          pl_table[residue],\n",
    "                          pka_table[residue], \n",
    "                          pkb_table[residue], \n",
    "                          pkx_table[residue]]\n",
    "    binary_properties = [1 if residue in acidic_charged_residues_table else 0,\n",
    "                         1 if residue in basic_charged_residues_table else 0, \n",
    "                         1 if residue in aromatic_residues_table else 0,\n",
    "                         1 if residue in aliphatic_residues_table else 0, \n",
    "                         1 if residue in polar_neutral_residues_table else 0]\n",
    "    return np.array(binary_properties + numeric_properties)\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    mol_size = mol.GetNumAtoms()\n",
    "\n",
    "    node_features = [] # Gather atom features\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        node_features.append(feature/sum(feature))\n",
    "\n",
    "    edges = []\n",
    "    for bond in mol.GetBonds(): # Getting the bonds and adding indices\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "        \n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    \n",
    "    edge_index = []\n",
    "    mol_adj = np.zeros((mol_size, mol_size)) # Adjacency matrix\n",
    "    for edge_1, edge_2 in g.edges:\n",
    "        mol_adj[edge_1, edge_2] = 1 # Fill in edges\n",
    "        \n",
    "    mol_adj += np.matrix(np.eye(mol_adj.shape[0]))\n",
    "    \n",
    "    row, col = np.where(mol_adj == 1) \n",
    "    for i, j in zip(row, col):\n",
    "        edge_index.append([i, j])\n",
    "    return mol_size, node_features, edge_index\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_not_k_encoding(atom.GetSymbol(), atom_table) +\n",
    "                    one_k_encoding(atom.GetDegree(), count_table) +\n",
    "                    one_not_k_encoding(atom.GetTotalNumHs(), count_table) +\n",
    "                    one_not_k_encoding(atom.GetImplicitValence(), count_table) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "# Metrics - MSE\n",
    "def get_mse(y_true, y_preds):\n",
    "    mse = ((y_true - y_pred)**2).mean(axis=0)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-93-f7669728a7bf>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-93-f7669728a7bf>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    for pair_ind in range(len(rows)):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation \n",
    "\n",
    "def create_dataset(dataset):\n",
    "    dataset_path = 'data/' + dataset + '/'\n",
    "    training = json.load(open(dataset_path + 'split/train_split.txt')) \n",
    "    training = [e for e in training]\n",
    "\n",
    "    proteins_ = json.load(open(dataset_path + 'proteins.txt'), object_pairs_hook=OrderedDict)\n",
    "    drugs_ = json.load(open(dataset_path + 'drugs.txt'), object_pairs_hook=OrderedDict)\n",
    "    \n",
    "    # Load contact map\n",
    "    contact_path = 'data/' + dataset + '/contact_maps'\n",
    "    contact_list = []\n",
    "    for key in proteins_:\n",
    "        contact_list.append(os.path.join(contact_path, key + '.npy'))\n",
    "\n",
    "    train = []\n",
    "    valid = training[0]\n",
    "    for i in range(len(training)):\n",
    "        if i != 0:\n",
    "            train += training[i]\n",
    "\n",
    "    interaction_scores = pickle.load(open(dataset_path + 'Y', 'rb'), encoding='latin1')\n",
    "    \n",
    "    # Lists \n",
    "    \n",
    "    drugs = []\n",
    "    proteins = []\n",
    "    protein_keys = []\n",
    "    drug_smiles = []\n",
    "    \n",
    "    # Convert SMILES\n",
    "    \n",
    "    for d in drugs_.keys():\n",
    "        mol = Chem.MolToSmiles(Chem.MolFromSmiles(drugs_[d]), isomericSmiles=True)\n",
    "        drugs.append(mol)\n",
    "        drug_smiles.append(drugs_[d])\n",
    "        \n",
    "    # Protein sequences \n",
    "    \n",
    "    for p in proteins_.keys():\n",
    "        proteins.append(proteins_[p])\n",
    "        protein_keys.append(p)\n",
    "        \n",
    "    # Need to transform the Davis scores \n",
    "    \n",
    "    if dataset == 'davis':\n",
    "        interaction_scores = [-np.log10(label/1e9) for label in interaction_scores]\n",
    "    interaction_scores = np.asarray(interaction_scores)\n",
    "    \n",
    "    train_count = 0\n",
    "    valid_count = 0\n",
    "    \n",
    "    # Training data\n",
    "    rows, cols = np.where(np.isnan(interaction_scores) == False) # Get rows with data\n",
    "    rows, cols = rows[train], cols[train]\n",
    "    train_entries = []\n",
    "    for pair_ind in range(len(rows)):\n",
    "        ls = []\n",
    "        ls += [drugs[rows[pair_ind]]]\n",
    "        ls += [proteins[cols[pair_ind]]]\n",
    "        ls += [protein_keys[cols[pair_ind]]]\n",
    "        ls += [interaction_scores[rows[pair_ind], cols[pair_ind]]]\n",
    "        train_entries.append(ls)\n",
    "        train_count += 1\n",
    "\n",
    "    csv_file = 'data/' + dataset + '_train'  + '.csv'\n",
    "    data_to_csv(csv_file, train_entries)\n",
    "            \n",
    "    # Validation data\n",
    "    rows, cols = np.where(np.isnan(interaction_scores) == False)\n",
    "    rows, cols = rows[valid], cols[valid]\n",
    "    valid_entries = []\n",
    "    for pair_ind in range(len(rows)):\n",
    "        ls = []\n",
    "        ls += [drugs[rows[pair_ind]]]\n",
    "        ls += [proteins[cols[pair_ind]]]\n",
    "        ls += [protein_keys[cols[pair_ind]]]\n",
    "        ls += [interaction_scores[rows[pair_ind], cols[pair_ind]]]\n",
    "        valid_entries.append(ls)\n",
    "        valid_count += 1\n",
    "\n",
    "    csv_file = 'data/' + dataset + '_valid'  + '.csv'\n",
    "    data_to_csv(csv_file, valid_entries)\n",
    "            \n",
    "    print('Dataset: ', dataset)\n",
    "    print('Working train entries:', train_count)\n",
    "    print('Working validation entries: ', valid_count)\n",
    "\n",
    "    compound_smiles = drugs\n",
    "    protein_key = protein_keys\n",
    "\n",
    "    # SMILES to graph\n",
    "    \n",
    "    smile_graph = {}\n",
    "    for smile in compound_smiles:\n",
    "        graph = smile_to_graph(smile)\n",
    "        smile_graph[smile] = graph\n",
    "\n",
    "    # Protein to graph \n",
    "    \n",
    "    protein_graph = {}\n",
    "    for key in protein_key:\n",
    "        # print(key)\n",
    "        graph = protein_to_graph(key, proteins_[key], contact_path)\n",
    "        protein_graph[key] = graph\n",
    "\n",
    "    train_csv = 'data/' + dataset + '_' + 'train' + '.csv'\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    \n",
    "    train_drugs, train_prot_keys, train_Y = list(df_train['compound_smiles']), list(df_train['protein_key']), list(df_train['interaction_scores'])\n",
    "    train_drugs, train_prot_keys, train_Y = np.asarray(train_drugs), np.asarray(train_prot_keys), np.asarray(train_Y)\n",
    "    train_dataset = ProtDrugDataset(root='data', dataset=dataset + '_' + 'train', xd=train_drugs, protein_key=train_prot_keys, label=train_Y, smile_graph=smile_graph, protein_graph=protein_graph)\n",
    "\n",
    "\n",
    "    df_valid = pd.read_csv('data/' + dataset + '_' + 'valid' + '.csv')\n",
    "    valid_drugs, valid_prots_keys, valid_Y = list(df_valid['compound_smiles']), list(df_valid['protein_key']), list(df_valid['interaction_scores'])\n",
    "    valid_drugs, valid_prots_keys, valid_Y = np.asarray(valid_drugs), np.asarray(valid_prots_keys), np.asarray(valid_Y)\n",
    "    valid_dataset = ProtDrugDataset(root='data', dataset=dataset + '_' + 'train', xd=valid_drugs,\n",
    "                               protein_key=valid_prots_keys, label=valid_Y, smile_graph=smile_graph,\n",
    "                               protein_graph=protein_graph)\n",
    "    \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object for PyTorch\n",
    "class ProtDrugDataset(InMemoryDataset):\n",
    "    def __init__(self, root, dataset, xd=None, label=None, smile_graph=None, protein_key=None, protein_graph=None):\n",
    "\n",
    "        super(ProtDrugDataset, self).__init__(root)\n",
    "        self.dataset = dataset\n",
    "        self.preprocess(xd, protein_key, label, smile_graph, protein_graph)\n",
    "\n",
    "    def preprocess(self, xd, protein_key, label, smile_graph, protein_graph):\n",
    "        data_list_mol = []\n",
    "        data_list_pro = []\n",
    "        data_len = len(xd)\n",
    "        \n",
    "        for i in range(data_len):\n",
    "            smiles = xd[i]\n",
    "            prot_key = protein_key[i]\n",
    "            labels = label[i]\n",
    "            mol_size, features, edge_index = smile_graph[smiles]\n",
    "            protein_size, protein_features, protein_edges = protein_graph[prot_key]\n",
    "\n",
    "            # Processing for GCN algorithms:\n",
    "            GCNData_mol = DATA.Data(x=torch.Tensor(features),\n",
    "                                    edge_index=torch.LongTensor(edge_index).transpose(1, 0),\n",
    "                                    y=torch.FloatTensor([labels]))\n",
    "            \n",
    "            GCNData_mol.__setitem__('mol_size', torch.LongTensor([mol_size]))\n",
    "\n",
    "            GCNData_pro = DATA.Data(x=torch.Tensor(protein_features),\n",
    "                                    edge_index=torch.LongTensor(protein_edges).transpose(1, 0),\n",
    "                                    y=torch.FloatTensor([labels]))\n",
    "            GCNData_pro.__setitem__('protein_size', torch.LongTensor([protein_size]))\n",
    "            \n",
    "            data_list_mol.append(GCNData_mol)\n",
    "            data_list_pro.append(GCNData_pro)\n",
    "\n",
    "        self.data_mol = data_list_mol\n",
    "        self.data_pro = data_list_pro\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_mol)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_mol[idx], self.data_pro[idx]\n",
    "\n",
    "# Training function \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    print('Beginning training.')\n",
    "    model.train()\n",
    "    \n",
    "    VERBOSE_INT = 10\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # To device \n",
    "        data_mol = data[0].to(device)\n",
    "        data_pro = data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data_mol, data_pro)\n",
    "        \n",
    "        loss = loss_fn(output, data_mol.y.view(-1, 1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % VERBOSE_INT == 0:\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                           batch_idx*TRAIN_BATCH_SIZE,\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100.*batch_idx/len(train_loader),\n",
    "                                                                           loss.item()))      \n",
    "# Prediction function \n",
    "def predict(model, device, loader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    \n",
    "    print('Beginning prediction.')\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data_mol = data[0].to(device)\n",
    "            data_pro = data[1].to(device)\n",
    "            output = model(data_mol, data_pro)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, data_mol.y.view(-1, 1).cpu()), 0)\n",
    "            \n",
    "    return total_labels.numpy().flatten(), total_preds.numpy().flatten()\n",
    "\n",
    "\n",
    "def collate(data_list):\n",
    "    batchA = Batch.from_data_list([data[0] for data in data_list])\n",
    "    batchB = Batch.from_data_list([data[1] for data in data_list])\n",
    "    return batchA, batchB\n",
    "\n",
    "def data_to_csv(csv_file, datalist):\n",
    "    with open(csv_file, 'w') as f:\n",
    "        f.write('compound_smiles,protein_sequence,protein_key,interaction_scores\\n')\n",
    "        for data in datalist:\n",
    "            f.write(','.join(map(str, data)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNNet Loaded\n",
      "Dataset:  davis\n",
      "Working train entries: 20036\n",
      "Working validation entries:  5010\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "model = GCNNet()\n",
    "model.to(device)\n",
    "model_name = 'Davis_GCN_1'\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_data, valid_data = create_dataset(dataset)\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training.\n",
      "Train epoch: 1 [0/20036 (0%)]\tLoss: 30.955589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-d7b5421b0f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c637a7d2a514>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_mol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_pro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_mol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d8d543feb4d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_mol, data_pro)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mx_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpro_conv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_edge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mx_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n\u001b[0;32m--> 169\u001b[0;31m                              size=None)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aggregate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoll_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mupdate_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoll_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n\u001b[0;32m--> 287\u001b[0;31m                            reduce=self.aggr)\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage_and_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "UPPER_BOUND = 99999\n",
    "LOWER_BOUND = -1\n",
    "\n",
    "best_mse = UPPER_BOUND\n",
    "best_test_mse =  UPPER_BOUND\n",
    "\n",
    "best_epoch = LOWER_BOUND\n",
    "\n",
    "model_file_name = 'models/model_' + model_name + '_' + dataset + '.model'\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        train(model, device, train_loader, optimizer, epoch + 1)\n",
    "        \n",
    "        print('Validation.')\n",
    "        G, P = predict(model, device, valid_loader)\n",
    "        val = get_mse(G, P)\n",
    "        \n",
    "        print('Validation result: ', val, best_mse)\n",
    "        \n",
    "        if val < best_mse:\n",
    "            best_mse = val\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('MSE improved! Epoch: ', best_epoch, '. MSE: ', best_mse)\n",
    "        else:\n",
    "            print('No improvement. Last improvement at epoch: ', best_epoch, '. MSE: ', best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load best saved model \n",
    "\n",
    "def load_model(path):\n",
    "    model = torch.load(path)\n",
    "    return model\n",
    "\n",
    "# Metrics for evaluating testing data\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    mse = get_mse(y_true, y_pred)\n",
    "    ci = get_ci(y_true, y_pred)\n",
    "    print('MSE: ', mse)\n",
    "    print('CI: ', ci)\n",
    "\n",
    "# Based on implementation/definition in: https://lifelines.readthedocs.io/en/latest/lifelines.utils.html#lifelines.utils.concordance_index\n",
    "def get_ci(y_true, y_pred):\n",
    "    admissible = 0\n",
    "    paired = 0\n",
    "    for i in range(1, len(y_true)):\n",
    "        for j in range(0, i):\n",
    "            if i != j:\n",
    "                if (y_true[i] > y_true[j]):\n",
    "                    paired += 1\n",
    "                    admissible += 1*(y_pred[i] > y_pred[j]) + 0.5*(y_pred[i] == y_pred[j])\n",
    "    if pair != 0:\n",
    "        return total/admissible\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(dataset):\n",
    "    \n",
    "    dataset_path = 'data/' + dataset + '/'\n",
    "    test = json.load(open(dataset_path + 'split/train_split.txt'))\n",
    "    \n",
    "    drugs_ = json.load(open(dataset_path + 'drugs.txt'), object_pairs_hook=OrderedDict)\n",
    "    proteins_ = json.load(open(dataset_path + 'proteins.txt'), object_pairs_hook=OrderedDict)\n",
    "    \n",
    "    interaction_scores = pickle.load(open(dataset_path + 'Y', 'rb'), encoding='latin1')\n",
    "    \n",
    "    contact_path = 'data/' + dataset + '/contact_maps'\n",
    "    contact_list = []\n",
    "    for key in proteins_:\n",
    "        contact_list.append(os.path.join(contact_path, key + '.npy'))\n",
    "\n",
    "    # Lists \n",
    "    \n",
    "    drugs = []\n",
    "    proteins = []\n",
    "    protein_keys = []\n",
    "    drug_smiles = []\n",
    "    \n",
    "    # Convert SMILES\n",
    "    \n",
    "    for d in drugs_.keys():\n",
    "        mol = Chem.MolToSmiles(Chem.MolFromSmiles(drugs_[d]), isomericSmiles=True)\n",
    "        drugs.append(mol)\n",
    "        drug_smiles.append(drugs_[d])\n",
    "        \n",
    "    # Protein sequences \n",
    "    \n",
    "    for p in proteins_.keys():\n",
    "        proteins.append(proteins_[p])\n",
    "        protein_keys.append(p)\n",
    "           \n",
    "    # Need to transform the Davis scores \n",
    "    \n",
    "    if dataset == 'davis':\n",
    "        interaction_scores = [-np.log10(label/1e9) for label in interaction_scores]\n",
    "    interaction_scores = np.asarray(interaction_scores)\n",
    "    \n",
    "    # Testing data\n",
    "    rows, cols = np.where(np.isnan(interaction_scores) == False) # Get rows with data\n",
    "    rows, cols = rows[test], cols[test]\n",
    "    test_entries = []\n",
    "    for pair_ind in range(len(rows)):\n",
    "        ls = []\n",
    "        ls += [drugs[rows[pair_ind]]]\n",
    "        ls += [proteins[cols[pair_ind]]]\n",
    "        ls += [protein_keys[cols[pair_ind]]]\n",
    "        ls += [interaction_scores[rows[pair_ind], cols[pair_ind]]]\n",
    "        test.append(ls)\n",
    "        test_count += 1\n",
    "        \n",
    "    csv_file = 'data/' + dataset + '_test' + '.csv'\n",
    "    data_to_csv(csv_file, test_entries)\n",
    "\n",
    "    compound_smiles = drugs\n",
    "    protein_key = protein_keys\n",
    "\n",
    "    # SMILES to graph\n",
    "    \n",
    "    smile_graph = {}\n",
    "    for smile in compound_smiles:\n",
    "        graph = smile_to_graph(smile)\n",
    "        smile_graph[smile] = graph\n",
    "\n",
    "    # Protein to graph \n",
    "    \n",
    "    protein_graph = {}\n",
    "    for key in protein_key:\n",
    "        # print(key)\n",
    "        graph = protein_to_graph(key, proteins_[key], contact_path)\n",
    "        protein_graph[key] = graph\n",
    "        \n",
    "    test_csv = 'data/' + dataset + '_' + 'test' + '.csv'\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    \n",
    "    test_drugs, test_prot_keys, test_Y = list(df_test['compound_smiles']), list(df_test['protein_key']), list(df_test['interaction_scores'])\n",
    "    test_drugs, test_prot_keys, test_Y = np.asarray(test_drugs), np.asarray(test_prot_keys), np.asarray(v_Y)\n",
    "    test_dataset = ProtDrugDataset(root='data', dataset=dataset + '_' + 'train', xd=test_drugs, protein_key=test_prot_keys, label=test_Y, smile_graph=smile_graph, protein_graph=protein_graph)\n",
    "\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNNet Loaded\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/model_Davis_GCN_3_davis.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-016a22210cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/model_Davis_GCN_3_davis.model'"
     ]
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 256\n",
    "\n",
    "model = GCNNet()\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_file_name, map_location='cuda:0'))\n",
    "\n",
    "test_data = create_test_data(dataset)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "y_true, y_preds = predicting(model, device, test_loader)\n",
    "calculate_metrics(y_true, y_preds, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
